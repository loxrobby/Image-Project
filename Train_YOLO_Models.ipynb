{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2f071f0",
   "metadata": {},
   "source": [
    "# Train YOLO Models in Google Colab\n",
    "\n",
    "**GitHub:** [Train and Deploy YOLO Models](https://github.com/EdjeElectronics/Train-and-Deploy-YOLO-Models)\n",
    "\n",
    "# Introduction\n",
    "\n",
    "This notebook uses [Ultralytics](https://docs.ultralytics.com/) to train YOLO11, YOLOv8, or YOLOv5 object detection models with a custom dataset. At the end of this Colab, you'll have a custom YOLO model that you can run on your PC, phone, or edge device like the Raspberry Pi.\n",
    "\n",
    "<p align=center>\n",
    "<img src=\"https://s3.us-west-1.amazonaws.com/evanjuras.com/img/yolo-model-demo.gif\" height=\"360\"><br>\n",
    "<i>Custom YOLO candy detection model in action!</i>\n",
    "</p>\n",
    "\n",
    "<p align=center>\n",
    "<a href=\"https://youtu.be/r0RspiLG260\" target=\"_blank\"><img src=\"https://raw.githubusercontent.com/EdjeElectronics/Train-and-Deploy-YOLO-Models/refs/heads/main/doc/Train_YOLO_Thumbnail2.png\" height=\"240\"><br>\n",
    "<i>Click here to go to the video!</i></a>\n",
    "</p>\n",
    "\n",
    "\n",
    "### Working in Colab\n",
    "Colab provides a virtual machine in your browser complete with a Linux OS, filesystem, Python environment, and best of all, a free GPU. We'll install PyTorch and Ultralytics in this environment and use it to train our model. Simply click the Play button on sections of code in this notebook to execute them on the virtual machine.\n",
    "\n",
    "### Navigation\n",
    "To navigate this notebook, use the table of contents in the left sidebar to jump from section to section.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b794235",
   "metadata": {},
   "source": [
    "**Verify NVIDIA GPU Availability**\n",
    "\n",
    "Make sure you're using a GPU-equipped machine by going to \"Runtime\" -> \"Change runtime type\" in the top menu bar, and then selecting one of the GPU options in the Hardware accelerator section. Click Play on the following code block to verify that the NVIDIA GPU is present and ready for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b95c496",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec122d5",
   "metadata": {},
   "source": [
    "#1.&nbsp;Gather and Label Training Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588b52e0",
   "metadata": {},
   "source": [
    "Before we start training, we need to gather and label images that will be used for training the object detection model. A good starting point for a proof-of-concept model is 200 images. The training images should have random objects in the image along with the desired objects, and should have a variety of backgrounds and lighting conditions.\n",
    "\n",
    "There are a couple options for gathering images:\n",
    "\n",
    "\n",
    "*   Build a custom dataset by taking your own pictures of the objects and labeling them (this typically results in the best performance)\n",
    "*   Find a pre-made dataset from sources like [Roboflow Universe](), [Kaggle](), or [Google Images V7]()\n",
    "\n",
    "\n",
    "If you want to build your own dataset, there are several tools available for labeling images. One good option is [Label Studio](https://labelstud.io/?utm_source=youtube&utm_medium=video&utm_campaign=edjeelectronics), a free and open-source labeling tool that has a simple workflow while providing capabilities for more advanced features. My YouTube video that walks through this notebook (link to be added soon) shows how to label images with Label Studio.\n",
    "\n",
    "<p align=center>\n",
    "<img src=\"https://raw.githubusercontent.com/EdjeElectronics/Train-and-Deploy-YOLO-Models/refs/heads/main/doc/label-studio-example.PNG\" height=\"380\"><br>\n",
    "<i>Example of a candy image labeled with Label Studio.</i>\n",
    "</p>\n",
    "\n",
    "If you used Label Studio to label and export the images, they'll be exported in a `project.zip` file that contains the following:\n",
    "\n",
    "- An `images` folder containing the images\n",
    "- A `labels` folder containing the labels in YOLO annotation format\n",
    "- A `classes.txt` labelmap file that contains all the classes\n",
    "- A `notes.json` file that contains info specific to Label Studio (this file can be ignored)\n",
    "\n",
    "If you obtained your dataset from another source (like Roboflow Universe) or used another tool to label your dataset, make sure the files are organized in the same folder structure.\n",
    "\n",
    "<p align=center>\n",
    "<img src=\"https://raw.githubusercontent.com/EdjeElectronics/Train-and-Deploy-YOLO-Models/refs/heads/main/doc/zipped-data-example.png\" height=\"\"><br>\n",
    "<i>Organize your data in the folders shown here. See my <a href=\"https://s3.us-west-1.amazonaws.com/evanjuras.com/resources/candy_data_06JAN25.zip\">Candy Detection Dataset</a> for an example.</i>\n",
    "</p>\n",
    "\n",
    "Once you've got your dataset built, put into the file structure shown above, and zipped into `data.zip`, you're ready to move on to the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84da682",
   "metadata": {},
   "source": [
    "# 2.&nbsp;Upload Image Dataset and Prepare Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d682e3e7",
   "metadata": {},
   "source": [
    "Next, we'll upload our dataset and prepare it for training with YOLO. We'll split the dataset into train and validation folders, and we'll automatically generate the configuration file for training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af63a21",
   "metadata": {},
   "source": [
    "## 2.1 Upload images\n",
    "\n",
    "First, we need to upload the dataset to Colab. Here are a few options for moving the `data.zip` folder into this Colab instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30a6b64",
   "metadata": {},
   "source": [
    "**Upload through Google Colab**\n",
    "\n",
    "Upload the `data.zip` file to the Google Colab instance by clicking the \"Files\" icon on the left hand side of the browser, and then the \"Upload to session storage\" icon. Select the zip folder to upload it.\n",
    "\n",
    "<p>\n",
    "<br>\n",
    "<img src=\"https://raw.githubusercontent.com/EdjeElectronics/Train-and-Deploy-YOLO-Models/refs/heads/main/doc/upload-colab-files.png\" height=\"240\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3844f0b2",
   "metadata": {},
   "source": [
    "## 2.2 Split images into train and validation folders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d212c8ba",
   "metadata": {},
   "source": [
    "Ultralytics requires a particular folder structure to store training data for models. The root folder is named â€œdataâ€. Inside, there are two main folders:\n",
    "\n",
    "*   **Train**: These are the actual images used to train the model. In one epoch of training, every image in the train set is passed into the neural network. The training algorithm adjusts the network weights to fit the data in the images.\n",
    "\n",
    "\n",
    "*   **Validation**: These images are used to check the model's performance at the end of each training epoch.\n",
    "\n",
    "In each of these folders is a â€œimagesâ€ folder and a â€œlabelsâ€ folder, which hold the image files and annotation files respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369c269b",
   "metadata": {},
   "source": [
    "\n",
    "I wrote a Python script that will automatically create the required folder structure and randomly move 90% of dataset to the \"train\" folder and 10% to the \"validation\" folder. Run the following code block to download and execute the scrpt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886c063c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O /content/train_val_split.py https://raw.githubusercontent.com/EdjeElectronics/Train-and-Deploy-YOLO-Models/refs/heads/main/utils/train_val_split.py\n",
    "\n",
    "# TO DO: Improve robustness of train_val_split.py script so it can handle nested data folders, etc\n",
    "!python train_val_split.py --datapath=\"/content/custom_data\" --train_pct=0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d35f9c9",
   "metadata": {},
   "source": [
    "# 3.&nbsp;Install Requirements (Ultralytics)\n",
    "\n",
    "Next, we'll install the Ultralytics library in this Google Colab instance. This Python library will be used to train the YOLO model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2234b69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b023e3",
   "metadata": {},
   "source": [
    "# 4.&nbsp;Configure Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b65117",
   "metadata": {},
   "source": [
    "There's one last step before we can run training: we need to create the Ultralytics training configuration YAML file. This file specifies the location of your train and validation data, and it also defines the model's classes. An example configuration file model is available [here](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/coco128.yaml).\n",
    "\n",
    "Run the code block below to automatically generate a `data.yaml` configuration file. Make sure you have a labelmap file located at `custom_data/classes.txt`. If you used Label Studio or one of my pre-made datasets, it should already be present. If you assembled the dataset another way, you may have to manually create the `classes.txt` file (see [here](https://github.com/EdjeElectronics/Train-and-Deploy-YOLO-Models/blob/main/doc/classes.txt) for an example of how it's formatted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44664a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python function to automatically create data.yaml config file\n",
    "# 1. Reads \"classes.txt\" file to get list of class names\n",
    "# 2. Creates data dictionary with correct paths to folders, number of classes, and names of classes\n",
    "# 3. Writes data in YAML format to data.yaml\n",
    "\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "def create_data_yaml(path_to_classes_txt, path_to_data_yaml):\n",
    "\n",
    "  # Read class.txt to get class names\n",
    "  if not os.path.exists(path_to_classes_txt):\n",
    "    print(f'classes.txt file not found! Please create a classes.txt labelmap and move it to {path_to_classes_txt}')\n",
    "    return\n",
    "  with open(path_to_classes_txt, 'r') as f:\n",
    "    classes = []\n",
    "    for line in f.readlines():\n",
    "      if len(line.strip()) == 0: continue\n",
    "      classes.append(line.strip())\n",
    "  number_of_classes = len(classes)\n",
    "\n",
    "  # Create data dictionary\n",
    "  data = {\n",
    "      'path': '/content/data',\n",
    "      'train': 'train/images',\n",
    "      'val': 'validation/images',\n",
    "      'nc': number_of_classes,\n",
    "      'names': classes\n",
    "  }\n",
    "\n",
    "  # Write data to YAML file\n",
    "  with open(path_to_data_yaml, 'w') as f:\n",
    "    yaml.dump(data, f, sort_keys=False)\n",
    "  print(f'Created config file at {path_to_data_yaml}')\n",
    "\n",
    "  return\n",
    "\n",
    "# Define path to classes.txt and run function\n",
    "path_to_classes_txt = '/content/custom_data/classes.txt'\n",
    "path_to_data_yaml = '/content/data.yaml'\n",
    "\n",
    "create_data_yaml(path_to_classes_txt, path_to_data_yaml)\n",
    "\n",
    "print('\\nFile contents:\\n')\n",
    "!cat /content/data.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f410cb1",
   "metadata": {},
   "source": [
    "# 5.&nbsp;Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e551a07",
   "metadata": {},
   "source": [
    "## 5.1 Training Parameters\n",
    "Now that the data is organized and the config file is created, we're ready to start training! \n",
    "\n",
    "**Model architecture & size (`model`):**\n",
    "\n",
    "There are several YOLO11 models sizes available to train, including `yolo11n.pt`, `yolo11s.pt`, `yolo11m.pt`, `yolo11l.pt`, and `yolo11xl.pt`. Larger models run slower but have higher accuracy, while smaller models run faster but have lower accuracy. If you aren't sure which model size to use, `yolo11s.pt` is a good starting point.\n",
    "\n",
    "You can also train YOLOv8 or YOLOv5 models by substituting `yolo11` for `yolov8` or `yolov5`.\n",
    "\n",
    "\n",
    "**Number of epochs (`epochs`)**\n",
    "\n",
    "In machine learning, one â€œepochâ€ is one single pass through the full training dataset. Setting the number of epochs dictates how long the model will train for. The best amount of epochs to use depends on the size of the dataset and the model architecture. If your dataset has less than 200 images, a good starting point is 60 epochs. If your dataset has more than 200 images, a good starting point is 40 epochs.\n",
    "\n",
    "\n",
    "**Resolution (`imgsz`)**\n",
    "\n",
    "Resolution has a large impact on the speed and accuracy of the model: a lower resolution model will have higher speed but less accuracy. YOLO models are typically trained and inferenced at a 640x640 resolution. However, if you want your model to run faster or know you will be working with low-resolution images, try using a lower resolution like 480x480.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eddf05d",
   "metadata": {},
   "source": [
    "## 5.2 Run Training!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f787b1",
   "metadata": {},
   "source": [
    "Run the following code block to begin training. If you want to use a different model, number of epochs, or resolution, change `model`, `epochs`, or `imgsz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd6471a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo detect train data=/content/data.yaml model=yolo11s.pt epochs=60 imgsz=640"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b7011a",
   "metadata": {},
   "source": [
    "The training algorithm will parse the images in the training and validation directories and then start training the model. At the end of each training epoch, the program runs the model on the validation dataset and reports the resulting mAP, precision, and recall. As training continues, the mAP should generally increase with each epoch. Training will end once it goes through the number of epochs specified by `epochs`.\n",
    "\n",
    "> **NOTE:** Make sure to allow training to run to completion, because an optimizer runs at the end of training that strips out unneeded layers from the model.\n",
    "\n",
    "The best trained model weights will be saved in `content/runs/detect/train/weights/best.pt`. Additional information about training is saved in the `content/runs/detect/train` folder, including a `results.png` file that shows how loss, precision, recall, and mAP progressed over each epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dfaed5",
   "metadata": {},
   "source": [
    "## 6. Test Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f141abd5",
   "metadata": {},
   "source": [
    "The model has been trained; now it's time to test it! The commands below run the model on the images in the validation folder and then display the results for the first 10 images. This is a good way to confirm your model is working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ae9523",
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo detect predict model=runs/detect/train/weights/best.pt source=data/validation/images save=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2bc558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from IPython.display import Image, display\n",
    "for image_path in glob.glob(f'/content/runs/detect/predict/*.jpg')[:10]:\n",
    "  display(Image(filename=image_path, height=400))\n",
    "  print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddfc5dd",
   "metadata": {},
   "source": [
    "The model should draw a box around each object of interest in each image. If it isn't doing a good job of detecting objects, here are a few tips:\n",
    "\n",
    "1. Double-check your dataset to make sure there are no labeling errors or conflicting examples.\n",
    "2. Increase the number of epochs used for training.\n",
    "3. Use a larger model size (e.g. `yolo11l.pt`).\n",
    "4. Add more images to the training dataset.\n",
    "\n",
    "You can also run the model on video files or other images images by uploading them to this notebook and using the above `!yolo detect predict` command, where `source` points to the location of the video file, image, or folder of images. The results will be saved in `runs/detect/predict`.\n",
    "\n",
    "Drawing boxes on images is great, but it isn't very useful in itself. It's also not very helpful to just run this models inside a Colab notebook: it's easier if we can just run it on a local computer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccae4f2",
   "metadata": {},
   "source": [
    "## 7. Deploy Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e00f7f",
   "metadata": {},
   "source": [
    "Now that your custom model has been trained, it's ready to be downloaded and deployed in an application! YOLO models can run on a wide variety of hardware, including PCs, embedded systems, and phones. Ultralytics makes it easy to convert the YOLO models to various formats (`tflite`, `onnx`, etc.) and deploy them in a variety of environments.\n",
    "\n",
    "This section shows how to download the model and provides links to instructions for deploying it on your PC and edge devices like the Raspberry Pi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afa81cd",
   "metadata": {},
   "source": [
    "## 7.1 Download YOLO Model\n",
    "\n",
    "First, zip and download the trained model by running the code blocks below.\n",
    "\n",
    "The code creates a folder named `my_model`, moves the model weights into it, and renames them from `best.pt` to `my_model.pt`. It also adds the training results in case you want to reference them later. It then zips the folder as `my_model.zip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013d8385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create \"my_model\" folder to store model weights and train results\n",
    "!mkdir /content/my_model\n",
    "!cp /content/runs/detect/train/weights/best.pt /content/my_model/my_model.pt\n",
    "!cp -r /content/runs/detect/train /content/my_model\n",
    "\n",
    "# Zip into \"my_model.zip\"\n",
    "%cd my_model\n",
    "!zip /content/my_model.zip my_model.pt\n",
    "!zip -r /content/my_model.zip train\n",
    "%cd /content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e63270",
   "metadata": {},
   "source": [
    "## 7.2 Deploy YOLO Model on Local Devices\n",
    "\n",
    "Next, we'll take our downloaded model and run it on a local device. This section provides instructions showing how to deploy YOLO models on various devices.\n",
    "\n",
    "I wrote a basic Python script, `yolo_detect.py`, that shows how to load a model, run inference on an image source, parse the inference results, and display boxes around each detected class in the image. The script gives an example of how to work with Ultralytics YOLO models in Python, and it can be used as a starting point for more advanced applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f318af89",
   "metadata": {},
   "source": [
    "# 8.&nbsp;Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b26e08b",
   "metadata": {},
   "source": [
    "Congratulations! You've successfully trained and deployed a YOLO object detection model. ðŸ˜€\n",
    "\n",
    "Next, you can extend your application beyond just drawing boxes and counting objects. Add functionality like logging the number of objects detected over time or taking a picture when certain objects are detected.\n",
    "\n",
    "Thanks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
